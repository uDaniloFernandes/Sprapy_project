# üï∑Ô∏è Projeto de Extra√ß√£o de Dados do SISAB com Scrapy

## 1. Resumo üéØ

Este projeto utiliza o framework Scrapy (Python) para automatizar a extra√ß√£o de relat√≥rios de produ√ß√£o do portal SISAB (Sistema de Informa√ß√£o em Sa√∫de para a Aten√ß√£o B√°sica). A solu√ß√£o foi desenhada para ser robusta e modular, superando o desafio de interagir com uma p√°gina din√¢mica que exige m√∫ltiplos passos para a obten√ß√£o dos dados.

O objetivo principal √© automatizar o download de arquivos CSV, selecionando dinamicamente as compet√™ncias (datas) de interesse e configurando m√∫ltiplos filtros, como estados, tipos de equipe e categorias profissionais.

## 2. Arquitetura Modular e L√≥gica de Heran√ßa üß±

A caracter√≠stica principal deste projeto √© sua arquitetura modular, que promove a separa√ß√£o de responsabilidades e a reutiliza√ß√£o de c√≥digo. Isso √© alcan√ßado atrav√©s de uma estrutura com dois spiders e o uso de heran√ßa.

### 2.1. `DateFinderSpider` (`get_dates.py`) üìÖ

Este √© o **spider base**. Sua √∫nica responsabilidade √© conectar-se ao portal do SISAB e extrair a lista completa de todas as compet√™ncias (datas) dispon√≠veis para consulta. Ele funciona como um "provedor de datas" para outros spiders.

- **Fun√ß√£o:** Acessar a p√°gina e obter a lista de datas.
- **Independ√™ncia:** Pode ser executado de forma independente (`scrapy crawl date_finder`) para uma verifica√ß√£o r√°pida das datas dispon√≠veis no portal.

### 2.2. `SisabSpider` (`sisab.py`) üì•

Este √© o **spider principal**, respons√°vel pela extra√ß√£o de fato. Ele cont√©m a l√≥gica de neg√≥cio para baixar os relat√≥rios.

- **Fun√ß√£o:** Filtrar as datas de interesse, montar a requisi√ß√£o POST com todos os par√¢metros necess√°rios e salvar o arquivo CSV resultante.
- **L√≥gica de Neg√≥cio:** √â neste spider que se configura a lista `datas_alvo` (os per√≠odos que se deseja baixar) e todos os outros filtros do relat√≥rio.

### 2.3. A L√≥gica de Heran√ßa üß¨

Para conectar os dois spiders, utilizamos o conceito de **heran√ßa** do Python:

```python
# Em sisab.py
from .get_dates import DateFinderSpider

class SisabSpider(DateFinderSpider):
    # ... c√≥digo do spider principal
```

**Como funciona o fluxo:**

1.  Ao executar `scrapy crawl sisab`, o Scrapy inicia o `SisabSpider`.
2.  Como `SisabSpider` **herda** de `DateFinderSpider`, ele automaticamente ganha todos os seus m√©todos, incluindo o `start_requests`.
3.  O `start_requests` do `DateFinderSpider` √© executado, fazendo a primeira requisi√ß√£o √† p√°gina.
4.  Ap√≥s a p√°gina ser baixada, o m√©todo `parse_page` do spider base extrai a lista de todas as datas dispon√≠veis.
5.  Em seguida, ele chama o m√©todo `process_dates(response, dates)`, que foi projetado para ser implementado pelo spider filho.
6.  O `SisabSpider` implementa sua pr√≥pria vers√£o do `process_dates`. √â aqui que a "m√°gica" acontece: ele recebe a lista completa de datas, aplica seus pr√≥prios filtros (`datas_alvo`), monta a requisi√ß√£o POST complexa e dispara o processo de download.

Essa abordagem garante que o `SisabSpider` n√£o precisa saber *como* obter as datas, ele apenas as recebe e decide *o que fazer* com elas. Se a forma de obter as datas no site mudar no futuro, apenas o `DateFinderSpider` precisar√° ser atualizado.

## 3. Como Executar üöÄ

**Pr√©-requisitos:**
- Python 3.x
- Scrapy (`pip install scrapy`)

**Executando o spider principal:**

Para realizar a extra√ß√£o completa e salvar os arquivos CSV, execute:

```sh
scrapy crawl sisab
```

Os arquivos ser√£o salvos no diret√≥rio configurado no m√©todo `save_csv` (atualmente `C:\Users\CintraMan\Downloads`).

**Executando o spider base:**

Para apenas listar as datas dispon√≠veis no portal, execute:

```sh
scrapy crawl date_finder
```

## 4. Configura√ß√£o ‚öôÔ∏è

As principais configura√ß√µes s√£o feitas no arquivo `Scrapy_project/spiders/sisab.py`:

-   **`datas_alvo`**: Modifique esta lista para definir quais per√≠odos (ano/m√™s) voc√™ deseja extrair.
-   **Filtros do Formul√°rio**: Dentro do m√©todo `process_dates`, o dicion√°rio `form_data` cont√©m todos os filtros enviados na requisi√ß√£o, como `estados`, `categoriaProfissional`, etc. Eles podem ser ajustados conforme a necessidade.
