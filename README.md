# üï∑Ô∏è Projeto de Extra√ß√£o de Dados do SISAB com Scrapy

## 1. Resumo üéØ

Este projeto utiliza o framework Scrapy (Python) para automatizar a extra√ß√£o de relat√≥rios de produ√ß√£o do portal SISAB. A solu√ß√£o foi desenhada para ser robusta e modular, e inclui uma API de gerenciamento (FastAPI) para controlar o processo de extra√ß√£o de forma ass√≠ncrona.

O objetivo √© permitir que um usu√°rio solicite uma extra√ß√£o de dados atrav√©s de uma API, acompanhe o progresso e baixe o arquivo final sem interagir diretamente com o processo de web scraping.

## 2. Arquitetura do Scrapy üß±

A caracter√≠stica principal do processo de extra√ß√£o √© sua arquitetura modular, que promove a separa√ß√£o de responsabilidades e a reutiliza√ß√£o de c√≥digo.

### 2.1. `DateFinderSpider` (`get_dates.py`) üìÖ

Este √© o **spider base**. Sua √∫nica responsabilidade √© conectar-se ao portal do SISAB e extrair a lista completa de todas as compet√™ncias (datas) dispon√≠veis para consulta. Ele funciona como um "provedor de datas" para outros spiders.

### 2.2. `SisabSpider` (`sisab.py`) üì•

Este √© o **spider principal**, respons√°vel pela extra√ß√£o de fato. Ele herda a capacidade de obter datas do `DateFinderSpider` e implementa a l√≥gica de neg√≥cio para baixar os relat√≥rios, como filtrar as datas de interesse e montar a requisi√ß√£o final.

### 2.3. A L√≥gica de Heran√ßa üß¨

O `SisabSpider` herda do `DateFinderSpider`, o que permite que ele reutilize a l√≥gica de conex√£o e obten√ß√£o de datas. Ao executar `scrapy crawl sisab`, o `start_requests` do spider base √© chamado, e as datas encontradas s√£o passadas para o m√©todo `process_dates` do spider principal, que continua a execu√ß√£o.

## 3. API de Gerenciamento (FastAPI) üë©‚Äçüíº

Para orquestrar o processo de extra√ß√£o, o projeto inclui uma API constru√≠da com FastAPI. A API atua como uma "gerente de tarefas", recebendo pedidos, delegando o trabalho pesado para o Scrapy e entregando o resultado final ao usu√°rio.

**A API n√£o executa a extra√ß√£o diretamente.** Ela gerencia um fluxo de trabalho ass√≠ncrono.

### L√≥gica das Rotas

-   #### `GET /`
    -   **Fun√ß√£o:** Rota de boas-vindas. Retorna uma mensagem simples e um link para a documenta√ß√£o interativa da API (`/docs`), servindo como ponto de partida para qualquer usu√°rio.

-   #### `POST /iniciar-extracao`
    -   **Fun√ß√£o:** Inicia um novo trabalho de extra√ß√£o.
    -   **L√≥gica:** Esta rota **n√£o** espera a extra√ß√£o terminar. Ela gera um identificador √∫nico para a tarefa (`task_id`), armazena o status inicial como `PENDENTE` e responde **imediatamente** ao usu√°rio, retornando o `task_id`. Em um sistema de produ√ß√£o, ela publicaria este `task_id` em uma fila de tarefas.

-   #### `GET /status/{task_id}`
    -   **Fun√ß√£o:** Verifica o progresso de um trabalho de extra√ß√£o.
    -   **L√≥gica:** O usu√°rio fornece o `task_id` recebido anteriormente. A API consulta seu banco de dados interno e retorna o status atual da tarefa (ex: `PENDENTE`, `CONCLUIDO` ou `ERRO`).

-   #### `GET /download/{task_id}`
    -   **Fun√ß√£o:** Entrega o arquivo CSV final.
    -   **L√≥gica:** Esta rota **n√£o** realiza o download do site do SISAB. Ela apenas verifica se um arquivo com o nome `Relatorio-SISAB_{task_id}.csv` j√° existe no disco (criado pelo spider Scrapy). Se o arquivo existir, a API o serve para o usu√°rio, iniciando o download no navegador.

## 4. Como Executar o Sistema üöÄ

### 4.1. Executando a API de Gerenciamento

No terminal, navegue at√© a pasta `api_service` e execute:

```sh
uvicorn main:app --reload
```

A API estar√° dispon√≠vel em `http://127.0.0.1:8000`. A documenta√ß√£o interativa pode ser acessada em `http://127.0.0.1:8000/docs`.

### 4.2. Executando a Extra√ß√£o (Scrapy)

Para executar o spider de forma independente (sem a API), navegue at√© a pasta `Scrapy_project` e execute:

```sh
scrapy crawl sisab
```

Para conectar a execu√ß√£o do Scrapy a uma tarefa da API, veja o fluxo de trabalho abaixo.

## 5. Fluxo de Trabalho Completo (Simula√ß√£o) üîÑ

Este guia simula como a API e o Scrapy trabalham juntos.

1.  **Inicie a API** conforme as instru√ß√µes acima.

2.  **Agende uma Tarefa:** Acesse a documenta√ß√£o (`/docs`), execute a rota `POST /iniciar-extracao` e **copie o `task_id`** retornado.

3.  **Execute o Worker (Scrapy):** Em outro terminal, na pasta `Scrapy_project`, execute o spider passando o `task_id` que voc√™ copiou. Este comando diz ao Scrapy para trabalhar em uma tarefa espec√≠fica:

    ```sh
    # Substitua {SUA_TASK_ID} pelo ID que voc√™ copiou
    scrapy crawl sisab -a task_id={SUA_TASK_ID}
    ```

4.  **Aguarde o Fim:** O Scrapy ir√° executar a extra√ß√£o e salvar o arquivo na sua pasta de Downloads com o nome `Relatorio-SISAB_{SUA_TASK_ID}.csv`.

5.  **Fa√ßa o Download pela API:** Agora que o arquivo existe, acesse a rota de download no seu navegador, usando a mesma `task_id`:

    ```
    http://127.0.0.1:8000/download/{SUA_TASK_ID}
    ```

    O download do arquivo CSV deve come√ßar imediatamente.

## 6. Configura√ß√£o ‚öôÔ∏è

As principais configura√ß√µes de extra√ß√£o s√£o feitas no arquivo `Scrapy_project/spiders/sisab.py`:

-   **`datas_alvo`**: Modifique esta lista para definir quais per√≠odos (ano/m√™s) voc√™ deseja extrair.
-   **Filtros do Formul√°rio**: Dentro do m√©todo `process_dates`, o dicion√°rio `form_data` cont√©m todos os filtros enviados na requisi√ß√£o.
