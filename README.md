# üï∑Ô∏è Projeto de Extra√ß√£o de Dados do SISAB com Scrapy

## 1. Resumo üéØ

Este projeto utiliza o framework Scrapy (Python) para automatizar a extra√ß√£o de relat√≥rios de produ√ß√£o do portal SISAB. A solu√ß√£o foi desenhada para ser robusta e modular, e inclui uma API de gerenciamento (FastAPI) que orquestra e executa o processo de web scraping de forma ass√≠ncrona.

O objetivo √© permitir que um usu√°rio solicite uma extra√ß√£o de dados atrav√©s de uma API, acompanhe o progresso e baixe o arquivo final, com a API gerenciando todo o ciclo de vida da extra√ß√£o.

## 2. Arquitetura do Scrapy üß±

A caracter√≠stica principal do processo de extra√ß√£o √© sua arquitetura modular, que promove a separa√ß√£o de responsabilidades e a reutiliza√ß√£o de c√≥digo.

### 2.1. `DateFinderSpider` (`get_dates.py`) üìÖ

Este √© o **spider base**. Sua √∫nica responsabilidade √© conectar-se ao portal do SISAB e extrair a lista completa de todas as compet√™ncias (datas) dispon√≠veis para consulta. Ele funciona como um "provedor de datas" para outros spiders.

### 2.2. `SisabSpider` (`sisab.py`) üì•

Este √© o **spider principal**, respons√°vel pela extra√ß√£o de fato. Ele herda a capacidade de obter datas do `DateFinderSpider` e implementa a l√≥gica de neg√≥cio para baixar os relat√≥rios, como filtrar as datas de interesse e montar a requisi√ß√£o final.

### 2.3. A L√≥gica de Heran√ßa üß¨

O `SisabSpider` herda do `DateFinderSpider`, o que permite que ele reutilize a l√≥gica de conex√£o e obten√ß√£o de datas. Ao ser iniciado pela API, o `start_requests` do spider base √© chamado, e as datas encontradas s√£o passadas para o m√©todo `process_dates` do spider principal, que continua a execu√ß√£o.

## 3. API de Gerenciamento (FastAPI) üë©‚Äçüíº

Para orquestrar o processo de extra√ß√£o, o projeto inclui uma API constru√≠da com FastAPI. Esta API atua como uma "gerente de tarefas" que recebe pedidos, **inicia e supervisiona a execu√ß√£o do Scrapy em segundo plano**, e entrega o resultado final ao usu√°rio.

**A API executa o Scrapy diretamente em um thread separado**, garantindo que a API permane√ßa responsiva.

### L√≥gica das Rotas

-   #### `GET /`
    -   **Fun√ß√£o:** Rota de boas-vindas. Redireciona para a documenta√ß√£o interativa da API (`/docs`), servindo como ponto de partida.

-   #### `POST /iniciar-extracao`
    -   **Fun√ß√£o:** Inicia um novo trabalho de extra√ß√£o.
    -   **L√≥gica:** Esta rota gera um identificador √∫nico para a tarefa (`task_id`), armazena o status inicial como `PENDENTE` no banco de dados e **inicia a execu√ß√£o do `SisabSpider` em um thread separado**. A API responde **imediatamente** ao usu√°rio, retornando o `task_id`.

-   #### `GET /status/{task_id}`
    -   **Fun√ß√£o:** Verifica o progresso de um trabalho de extra√ß√£o.
    -   **L√≥gica:** O usu√°rio fornece o `task_id`. A API consulta seu banco de dados e retorna o status atual da tarefa (ex: `PENDENTE`, `EM_PROGRESSO`, `CONCLUIDO` ou `ERRO`).

-   #### `GET /download/{task_id}`
    -   **Fun√ß√£o:** Entrega o arquivo CSV final.
    -   **L√≥gica:** Esta rota verifica o status da tarefa no banco de dados. Se a tarefa estiver `CONCLUIDO`, ela verifica se um arquivo com o nome `Relatorio-SISAB_{task_id}.csv` existe no disco persistente (criado pelo spider Scrapy). Se o arquivo existir, a API o serve para o usu√°rio, iniciando o download no navegador.

## 4. Como Executar o Sistema üöÄ

### 4.1. Executando a API (Localmente)

No terminal, navegue at√© a pasta `api_service` e execute:

```sh
uvicorn main:app --reload
```

A API estar√° dispon√≠vel em `http://127.0.0.1:8000`. A documenta√ß√£o interativa pode ser acessada em `http://127.0.0.1:8000/docs`.

### 4.2. Executando a Extra√ß√£o (Scrapy Localmente, para Teste)

Para testar o spider Scrapy de forma isolada (sem a API), navegue at√© a pasta `Scrapy_project` e execute:

```sh
scrapy crawl sisab
```

Este comando far√° o spider rodar e salvar√° um arquivo com um ID aleat√≥rio na sua pasta de Downloads.

## 5. Fluxo de Trabalho Completo (Usando a API) üîÑ

Este guia mostra como usar a API para iniciar e gerenciar uma extra√ß√£o.

1.  **Inicie a API** conforme as instru√ß√µes acima.

2.  **Inicie uma Extra√ß√£o:** Acesse a documenta√ß√£o (`/docs`), execute a rota `POST /iniciar-extracao` e **copie o `task_id`** retornado.

3.  **Monitore o Status:** Use a rota `GET /status/{task_id}` (na documenta√ß√£o ou diretamente no navegador) para verificar o progresso. O status mudar√° de `PENDENTE` para `EM_PROGRESSO` e, finalmente, para `CONCLUIDO` (ou `ERRO`).

4.  **Fa√ßa o Download:** Quando o status for `CONCLUIDO`, use a rota `GET /download/{task_id}` (na documenta√ß√£o ou diretamente no navegador) para baixar o arquivo CSV. O download do arquivo deve come√ßar imediatamente.

## 6. Configura√ß√£o ‚öôÔ∏è

As principais configura√ß√µes de extra√ß√£o s√£o feitas no arquivo `Scrapy_project/Scrapy_project/spiders/sisab.py`:

-   **`datas_alvo`**: Modifique esta lista para definir quais per√≠odos (ano/m√™s) voc√™ deseja extrair.
-   **Filtros do Formul√°rio**: Dentro do m√©todo `process_dates`, o dicion√°rio `form_data` cont√©m todos os filtros enviados na requisi√ß√£o.
